<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Rui Zhao, Rui Zhao, 赵瑞, EE, USTC, CUHK, The Chinese University of Hong Kong, University of Science and Technology of China" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="rui.ico">
<title>Rui Zhao's Homepage</title>
</head>
<body>
<div id="layout-content">
<p>

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

</p>

<p>
</p>
<table class="imgtable"><tr><td>
<img src="figures/rui@seoul_half.jpeg" alt="alt text" width="260px" height="HEIGHTpx" /> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
  <h1>
  <a href="http://www.ee.cuhk.edu.hk/~rzhao/">Rui Zhao</a> &nbsp; 趙瑞
  </h1>
</div>

<p>
<!-- Ph.D. Candidate, supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Prof. Xiaogang Wang</a> -->
Executive Research Director <br />
SenseTime Group Limited

<br />
<br />



Tel.: +86 188 2316 1367 <br />

Email: <a href="mailto:zhaorui@sensetime.com">zhaorui at sensetime.com</a><br />

12/F, Block C, ISPSZHK, Bing Lang Road, <br />

Futian District, Shenzhen, Guangdong, P.R.China
</p>
</td></tr></table>

&nbsp;&nbsp;[<a href="http://scholar.google.com/citations?user=1c9oQNMAAAAJ">Google Scholar</a>] [<a href="http://hk.linkedin.com/pub/rui-zhao/27/70a/323/">Linkedin</a>] [<a href="cv_rui.pdf">CV</a>]





<!-- <h2>
  Biography 
</h2> -->

<br /><br />


<p style="font-family: georgia,garamond,serif;font-size:14px;font-style:italic;">
Rui Zhao is an Executive Research Director at SenseTime, CTO and the head of R&D in SenseTime Smart City Group (SCG), where he leads a team of talented and energetic researchers and engineers to foster and develop innovative techniques for smart city solutions. Prior to this, he was the CTO and Chief Scientist of SenseNets, an affiliated venture with SenseTime during 2015-2017. He received his Bachelors degree from <a href="http://en.ustc.edu.cn/"> University of Science and Technology of China (USTC)</a> in 2010, his Ph.D. degree from <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong (CUHK)</a> in 2015. He is a leading expert on AI in both classic models (face recognition and person re-identification) and generative models (large language models and visual-language models). He has gained over 10,000 Google Scholar Citations with more than 150 publications during last 10 years. His research interests include computer vision, large language modeling, and large multimodality modeling. He is a recipient of <a href="http://www.wuwenjunkejijiang.cn/wj/news.aspx?pkid=14026&tid=13390">Wen Jun Wu's AI Award for Scientific and Technology Progress</a> from Ministry of Education of China in 2019, Black Sheep Award (for extraordinary contribution with 1‰ award rate) from SenseTime in 2018, Peacock Program Talent Award from Shenzhen Municipal Government in 2016, Outstanding Reviewer Award from ECCV 2016, Doctoral Consortium Award from CVPR 2015, and Travel Award from ICCV 2013 respectively.  

In SenseTime SCG, his team focuses on research topics including deep learning based human biometric analysis, scaling up large vision models, and large multimodality model applications, etc. His team has a close collaboration with Prof. <a href="https://wlouyang.github.io/">Wanli Ouyang</a> in Univ. of Sydney, Prof. <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a> in CUHK, Prof. <a href="http://mmlab.siat.ac.cn/yuqiao/index.html">Yu Qiao</a> in SIAT of CAS, Prof. <a href="http://staff.ustc.edu.cn/~ynh/">Nenghai Yu</a> in USTC, and Prof. <a href="http://dahua.me/">Dahua Lin</a> in CUHK. 

<br />
<br />
If you are looking to join SenseTime SCG for an Internship or as a full-time researcher, <a href = "mailto:safecityrecruit@sensetime.com">send an email with your resume</a>. We still have a few open positions. Candidates with strong publication record and self-motivation are prefered. 

</p>
<!-- <li>
  <p>
    I finished my Ph.D. thesis in <a href="http://www.ee.cuhk.edu.hk/">Department of Electronic Engineering</a> of <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong (CUHK)</a>. My supervisor is <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Prof. Xiaogang Wang</a>, and I also closely work with <a href="http://www.ee.cuhk.edu.hk/~wlouyang/">Prof. Wanli Ouyang</a>. I am a member of both <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Lab (MMLab)</a> and <a href="http://ivp.ee.cuhk.edu.hk/">Image and Video Processing Lab (IVPLab)</a>.  
  </p>
</li> -->

<!-- <li>
  <p>
    Rui Zhao is an Executive Research Director at SenseTime and head of SafeCity R&D Group in SenseTime, where he leads a team of talented and energetic researchers and engineers to foster and develop innovative techniques for smart city solutions. Prior to this, he was a CTO and Vice President of SenseNets, an affiliated venture of SenseTime. He received his Bachelors degree from <a href="http://en.ustc.edu.cn/"> University of Science and Technology of China (USTC)</a> in 2010, his Ph.D. degree from <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong (CUHK)</a> in 2015. He is a leading expert in AI for face recognition and person re-identification. He has gained over 4,500 Google Scholar Citations with recent 10 high impact publications during last few years. His research interests include machine learning, computer vision, and AGI. He is a recipient of Wen Jun Wu's AI Award for Scientific and Technology Progress in 2019, Black Sheep Award (for extraordinary contribution with 1‰ award ratio) from SenseTime in 2018, Peacock Program Talent Award in 2016, Outstanding Reviewer Award from ECCV 2016, Doctoral Consortium Award from CVPR 2015, and Travel Award from ICCV 2013 respectively.  

    
    His team focuses on research topics including large-scale network architects and training, unsupervised and semi-supervised learning, human biometric analysis, and scene understanding etc.    
  </p>
</li>

<li>
  <p>
    I obtained my Ph.D. in Electronic Engineering at <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong (CUHK)</a>, advised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>. I received the Bachelor degree in Electronic Engineering and Information Science (EEIS) at <a href="http://en.ustc.edu.cn/"> University of Science and Technology of China (USTC)</a>.   
  </p>
</li> -->


<h2>Highlights and News</h2>
<ul>
<!-- <li>
  <p>
      <font size="3" color="red"> I am recruiting self-motivated researcher and software engineers with expertise in deep learning and computer vision. Both full-time and intern positions are available. If interested, please email me with your CV. </font>    
  </p>
  <p>
      <font size="3" color="blue"> If you are looking to join SenseTime SRG for an Internship or as a full-time researcher, apply <a href="safecityrecruit@sensetime.com">here</a>. We have several open positions. Candidates with strong publication record and self-motivation are prefered. </font>    
  </p>
</li>
 -->

<li>
  <p>
      Nov. 2, 2020: one paper accepted by NIPS 2020, and one accepted by WACV 2021.   
  </p>
</li>


<li>
  <p>
      Jul. 3, 2020: two papers accepted by ECCV 2020.   
  </p>
</li>


<li>
  <p>
      Feb. 27, 2020: three papers accepted by CVPR 2020.   
  </p>
</li>


<li>
  <p>
      June, 2019: one papers accepted by ICCV 2019.   
  </p>
</li>

<li>
  <p>
      Mar. 20, 2019: two papers accepted by CVPR 2019.   
  </p>
</li>

<li>
  <p>
      Feb. 20, 2018: one paper accepted by CVPR 2018.   
  </p>
</li>

<li>
  <p>
      Jul. 23, 2016: one paper accepted by ECCV 2016.   
  </p>
</li>

<li>
  <p>
      Mar. 13, 2016: PAMI paper accepted.   
  </p>
</li>

<!-- <li>
  <p>
      Sep. 1, 2015: I joined <a href="http://www.sensetime.com/en">SenseTime Group Ltd.</a> as a senior researcher.
  </p>
</li> -->

<li>
  <p>
      May 21, 2015: <a href="https://github.com/Robert0812/deepsaldet">Code</a> and <a href="project/deepsal_cvpr15/zhaoOLWcvpr15.html">supplementary material</a> of our CVPR'15 work on saliency detection are released. 
  </p>
</li>

<!-- <li>
  <p>
      Apr. 7, 2015: I received travel award from the Doctoral Consortium of CVPR 2015!
  </p>
</li>

<li>
  <p>
      Jan. 08, 2015: I'm now at the end of my PhD and looking for a place where my skills and passion will have the highest positive impact. <a href="http://www.sensetime.com/en/tech/surveillance">(Check my CV!)</a>
  </p>
</li>

<li>
  <p>
      Nov. 21, 2013: I am awarded travel grant to ICCV 2013!
  </p>
</li>

<li>
  <p>
    May 22, 2013: Project page of our CVPR'13 work is built, and code is available. 
  </p>
</li>

<li>
  <p>May 15, 2013: New homepage launched!
  </p>
</li> -->

</ul>

<h2>Selected Publications [<a href="publications.html">Full List</a>]</h2> 
<ul>

<li>
<a href=""> Memory-Based Neighbourhood Embedding for Visual Recognition, </a> <br />
Suichan Li, Dapeng Chen, Bin Liu, Nenghai Yu, <b>Rui Zhao</b> <br />
<i>IEEE International Conference on Computer Vision </i> (<b>ICCV</b>), 2019. (Oral with acceptance rate: 4.6%) <br />
</li>
[<a href="https://arxiv.org/pdf/1908.04992">PDF</a>]
[<a href="javascript:toggleBibtex('LiCLYZiccv19_abstract')" target="_self">Abstract</a>]
<div class="blockcontent" id="LiCLYZiccv19_abstract" style="display:none"> 
  <table class="imgtable">
    <tr>
      <tr><p style="font-size:16px">
          Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, ie, how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.
      </p></tr>
      <tr><a href="project/gcnmemo_iccv19/LiCLYZiccv19.png"><img src="project/gcnmemo_iccv19/LiCLYZiccv19.png" alt="alt text" width="450px" height="HEIGHTpx"/></a></tr>
    </tr>
  </table>
</div>
<br /><br />

<li>
<a href=""> Saliency Detection by Multi-context Deep Learning, </a> <br />
<b>R. Zhao</b>, W. Ouyang, H. Li and X. Wang <br />
<i>IEEE International Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2015. (Acceptance rate: 28.4%) <br />
</li>
[<a href="project/deepsal_cvpr15/zhaoOLWcvpr15.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('ZhaoOLWcvpr15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ZhaoOLWcvpr15')" target="_self">Bibtex</a>]
[<a href="https://github.com/Robert0812/deepsaldet">Code</a>]
[<a href="project/deepsal_cvpr15/zhaoOLWcvpr15.html">Supplementary Material</a>]
[<a href="figures/coming_soon.jpg">Poster</a>]
<!-- [<a href="project/deepsal_cvpr15/zhaoOLWcvpr15.html">Project Page</a>] -->
[<a href="figures/coming_soon.jpg">DOI</a>]
<div class="blockcontent" id="ZhaoOLWcvpr15_abstract" style="display:none"> 
  <table class="imgtable">
    <tr>
      
      <tr><p style="font-size:16px">
          Low-level saliency cues or priors do not produce good enough saliency detection results especially when the salient object presents in a low-contrast background with confusing visual appearance. This issue raises a serious problem for conventional approaches. In this paper, we tackle this problem by proposing a multi-context deep learning framework for salient object detection. We employ deep Convolutional Neural Networks to model saliency of objects in images. Global context and local context are both taken into account, and are jointly modeled in a unified multi-context deep learning framework.
<br />
          To provide a better initialization for training the deep neural networks, we investigate different pre-training strategies, and a task-specific pre-training scheme is designed to make the multi-context modeling suited for saliency detection. Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated. Our approach is extensively evaluated on five public datasets, and experimental results show significant and consistent improvements over the state-of-the-art methods.
      </p></tr>
      <tr><a href="project/deepsal_cvpr15/zhaoOLWcvpr15.png"><img src="project/deepsal_cvpr15/zhaoOLWcvpr15.png" alt="alt text" width="450px" height="HEIGHTpx"/></a></tr>
    </tr>
  </table>
</div>

<div class="blockcontent" id="ZhaoOLWcvpr15" style="display:none"> 
<pre>
@inproceedings{zhao2015saliency,
 title = {Saliency Detection by Multi-context Deep Learning},
 author={Zhao, Rui and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
 booktitle={CVPR},
 year={2015}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://arxiv.org/abs/1412.4526"> Highly Efﬁcient Forward and Backward Propagation of Convolutional Neural
Networks for Pixelwise Classiﬁcation, </a> <br />
H. Li, <b>R. Zhao</b>, and X. Wang <br />
<i>Preprint </i>, arXiv:1412.4526, 2014. <br />
</li>
[<a href="http://arxiv.org/pdf/1412.4526v2.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('LiZWarxiv_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('LiZWarxiv')" target="_self">Bibtex</a>]
[<a href="http://arxiv.org/abs/1412.4526">DOI</a>]
<div class="blockcontent" id="LiZWarxiv_abstract" style="display:none"> 
  <table class="imgtable">
    <tr>
      
      <tr><p style="font-size:16px">
          We present highly efficient algorithms for performing forward and backward propagation of Convolutional Neural Network (CNN) for pixelwise classification on images. For pixelwise classification tasks, such as image segmentation and object detection, surrounding image patches are fed into CNN for predicting the classes of centered pixels via forward propagation and for updating CNN parameters via backward propagation. However, forward and backward propagation was originally designed for whole-image classification. Directly applying it to pixelwise classification in a patch-by-patch scanning manner is extremely inefficient, because surrounding patches of pixels have large overlaps, which lead to a lot of redundant computation. 
          The proposed algorithms eliminate all the redundant computation in convolution and pooling on images by introducing novel d-regularly sparse kernels. It generates exactly the same results as those by patch-by-patch scanning. Convolution and pooling operations with such kernels are able to continuously access memory and can run efficiently on GPUs. A fraction of patches of interest can be chosen from each training image for backward propagation by applying a mask to the error map at the last CNN layer. Its computation complexity is constant with respect to the number of patches sampled from the image. Experiments have shown that our proposed algorithms speed up commonly used patch-by-patch scanning over 1500 times in both forward and backward propagation. The speedup increases with the sizes of images and patches.
      </p></tr>

      <tr><a href="project/conv_cvpr15/LiZWarxiv.png"><img src="project/conv_cvpr15/LiZWarxiv.png" alt="alt text" width="600px" height="HEIGHTpx"/></a></tr>
    </tr>
  </table>
</div>

<div class="blockcontent" id="LiZWarxiv" style="display:none"> 
<pre>
@inproceedings{li2015high,
 title = {Highly Efﬁcient Forward and Backward Propagation of Convolutional Neural
          Networks for Pixelwise Classiﬁcation},
 author={Li, Hongsheng and Zhao, Rui and Wang, Xiaogang},
 booktitle={arXiv:1412.4526},
 year={2015}
}
</pre>
</div>
<br /><br />


<li>
<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6909421">DeepReid: Deep Filter Pairing Neural Network for Person Re-Identification, </a> <br />
W. Li, <b>R. Zhao</b>, T. Xiao and X. Wang. <br />
<i>IEEE International Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2014. (Acceptance rate: 29.8%)<br />
</li>
[<a href="project/fpnn_cvpr14/liZXWcvpr14.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('LiZXWcvpr14_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('LiZXWcvpr14')" target="_self">Bibtex</a>]
[<a href="project/fpnn_cvpr14/liZXWcvpr14_poster.pdf">Poster</a>]
[<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6909421">DOI</a>]
<div class="blockcontent" id="LiZXWcvpr14_abstract" style="display:none"> 
  <table class="imgtable">
    <tr>
      
      <tr><p style="font-size:16px">
          Person re-identification is to match pedestrian images from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image resolutions, camera settings, occlusions and background clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing person re-identification methods that use manually cropped pedestrian images and assume perfect detection.
          In this paper, we propose a novel filter pairing neural network (FPNN) to jointly handle misalignment, photometric and geometric transforms, occlusions and background clutter. All the key components are jointly optimized to maximize the strength of each component when cooperating with others. In contrast to existing works that use handcrafted features, our method automatically learns features optimal for the re-identification task from data. The learned filter pairs encode photometric transforms. Its deep architecture makes it possible to model a mixture of complex photometric and geometric transforms. We build the largest benchmark re-id dataset with 13,164 images of 1,360 pedestrians. Unlike existing datasets, which only provide manually cropped pedestrian images, our dataset provides automatically detected bounding boxes for evaluation close to practical applications. Our neural network significantly outperforms state-of-the-art methods on this dataset.
      </p></tr>

      <tr><a href="project/fpnn_cvpr14/liZXWcvpr14.png"><img src="project/fpnn_cvpr14/liZXWcvpr14.png" alt="alt text" width="600px" height="HEIGHTpx"/></a></tr>
    </tr>
  </table>
</div>

<div class="blockcontent" id="LiZXWcvpr14" style="display:none"> 
<pre>
@inproceedings{li2014deepreid,
 title = {DeepReid: Deep Filter Pairing Neural Network for Person Re-identification},
 author={Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2014},
 month = {June},
 address = {Columbus, USA}
}
</pre>
</div>
<br /><br />


<li>
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6619304">Unsupervised Salience Learning for Person Re-Identification, </a> <br />
<b>R. Zhao</b>, W. Ouyang and X. Wang. <br />
<i>IEEE International Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2013. (Acceptance rate: 25.2%)<br />
</li>
[<a href="project/salience_cvpr13/zhaoOWcvpr13.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('zhaoOWcvpr13_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('zhaoOWcvpr13')" target="_self">Bibtex</a>]
[<a href="http://mmlab.ie.cuhk.edu.hk/projects/project_salience_reid/index.html">Project Page</a>]
[<a href="project/salience_cvpr13/zhaoOWcvpr13_poster.pdf">Poster</a>]
[<a href= https://github.com/Robert0812/salience_reid>Code</a>]
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6619304">DOI</a>]
<div class="blockcontent" id="zhaoOWcvpr13_abstract" style="display:none"> 
  <table class="imgtable">
    <tr>
      <td><a href="project/salience_cvpr13/zhaoOWcvpr13.png"><img src="project/salience_cvpr13/zhaoOWcvpr13.png" alt="alt text" width="200px" height="HEIGHTpx"/></a></td>
      <td><p style="font-size:16px">
          Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identiﬁcation based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identiﬁcation, human salience is incorporated in patch matching to ﬁnd reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.
      </p></td>
    </tr>
  </table>
</div>

<div class="blockcontent" id="zhaoOWcvpr13" style="display:none"> 
<pre>
@inproceedings{zhao2013unsupervised,
 title = {Unsupervised Salience Learning for Person Re-identification},
 author={Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2013},
 month = {June},
 address = {Portland, USA}
}
</pre>
</div>

</ul>

<h2>
  Professional Activitiy
</h2>
  <p><i>Journal Reviewer</i></p>
  <ul>
    <li>TPAMI, IJCV, TIP, TCSVT, TNNLS, TITS, SPL, IMAVIS, MVA, etc</li>
  <!-- <li><p>IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI)</p></li>
  <li><p>IEEE Transaction on Image Processing (TIP)</p></li>
  <li><p>IEEE Transactions on Neural Network and Learning System (TNNLS)</p></li>
  <li><p>IEEE Transaction on Intelligent Transportation Systems (TITS)</p></li>
  <li><p>IEEE Transaction on Circuits and Systems for Video Technology (TCSVT)</p></li>
  <li><p>IEEE Signal Processing Letters (SPL)</p></li>
  <li><p>Elsevier Journal of Neurocomputing (NEUCOM) </p></li>     
  <li><p>Elsevier Journal of Image and Vision Computing (IMAVIS)</p></li>
  <li><p>Elsevier Journal of Visual Communication and Image Representation Registration (JVCI)</p></li>
  <li><p>Springer Journal of Machine Vision and Applications (MVA)</p></li>
  <li><p>Springer Journal of The Visual Computer (TVC)</p></li>
  <li><p>Springer Book "Person Re-identification", 2014<p></li> -->
  </ul>
  <!-- &nbsp; IEEE Transactions on Image Processing (TIP)<br />
  &nbsp; IEEE Transactions on Circuit System and Video Technology (TCSVT)<br />
  &nbsp; IEEE Transactions on Neural Network and Learning System (TNNLS)<br />
  &nbsp; IEEE Transactions on Intelligent Transportation Systems (TITS)<br />
  &nbsp; Elsevier Journal of Neuro Computing<br />
  &nbsp; Elsevier Journal of Image and Vision Computing<br />
  &nbsp; Elsevier Journal of Visual Communication and Image Representation <br />
  &nbsp; Springer Book of "Person Re-Identification" <br />
  &nbsp; Springer Journal of Machine Vision and Applications <br /><br /> -->

  <p><i>Conference Reviewer / External Reviewer</i></p>
  <ul><li>CVPR 2013-2018, ICCV 2015, ECCV 2014, ACCV 2014, ICPR 2012</li></ul>
  <p><i>Program Committee Member</i></p>
  <ul><li>AAAI 2016-2018, ACCV 2014, HIS (ACCV-W 2014), DLVD (ACCV-W 2014), BigMM 2015</li></ul>

  <!-- <p>Reviewer of IEEE Transactions (TIP, TCSVT, TITS, TNNLS, SPL), Elsevier Journals (NEUCOM, IMAVIS, JVCI), Springer Book (Person Re-Identification), Springer Journal (MVA), ICCV 2015, CVPR 2015, ACCV 2014, HIS (ACCV-W 2014), DLVD (ACCV-W 2014)</p> -->
    <!-- <ul>
      <li><p>IEEE Transaction on Image Processing (TIP)</p></li>
      <li><p>IEEE Transaction on Intelligent Transportation Systems (TITS)</p></li>
      <li><p>IEEE Transaction on Circuits and Systems for Video Technology (TCSVT)</p></li>
      <li><p>Elsevier Journal of Neurocomputing (NEUCOM) </p></li>     
      <li><p>Elsevier Journal of Image and Vision Computing (IMAVIS)</p></li>
      <li><p>Elsevier Journal of Visual Communication and Image Representation Registration (JVCI)</p></li>
      <li><p>Springer Book "Person Re-identification", 2014<p></li>
      <li><p>Springer Journal of Machine Vision and Applications (MVA)</p></li>
      <li><p>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2015</p></li>
      <li><p>Asian Conference of Computer Vision (ACCV), 2014</p></li>
      <li><p>ACCV Workshop on Human Identification for Surveillance (HIS), 2014</p></li>
      <li><p>ACCV Workshop on Deep Learning on Visual Data (DLVD), 2014</p></li>
  </ul> -->
  <!-- <p>Program Committee Member of BigMM 2015, ACCV 2014, <a href="https://sites.google.com/site/dlvd2014/home">DLVD</a> (ACCV-W 2014)</p> -->
  <!-- <ul>
    <li><p>Asian Conference on Computer Vision (ACCV), 2014</p></li>
    <li><p>ACCV Workshop on <a href="https://sites.google.com/site/dlvd2014/home">Deep Learning on Visual Data (DLVD)</a>, 2014</p></li>
  </ul> -->
  <!-- <p>Volunteer, ICIP 2010</p> -->
  <p>Member of IEEE and CVF</p>
  <!-- <p>Member, </p> -->


<h2>
  Teaching
</h2>

Teaching assistant at CUHK for the following courses:
<ul>
<li><p>2011, Fall &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Basic Circuit Theory (ENGG1110A). <br />
</p>
</li>
<li><p>2012, Spring &nbsp;&nbsp;&nbsp;&nbsp; Introduction to Engineering Design (ENGG1100). <br />
</p>
</li>
<li><p>2012, Fall &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Basic Circuit Theory (ENGG1110B). <br />
</p>
</li>
<li><p>2013, Spring &nbsp;&nbsp;&nbsp;&nbsp; Problem Solving By Java Programming (ENGG1110D). <br />
</p>
</li>
<li>
<p>
2014, Spring &nbsp;&nbsp;&nbsp;&nbsp; Problem Solving By Java Programming (ENGG1110J). <br />
</p>
</li>

<li>
  <p>

  2014, Spring &nbsp;&nbsp;&nbsp;&nbsp; <a href="https://github.com/Robert0812/Tutorial_PR">Pattern Recognition (ENGG5202)</a>. <br />
  </p>
</li>

<li>
<p>
2014, Fall &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Biomedical Imaging Applications (BMEG4320). <br />
</p>
</li>

<li>
<p>
2015, Spring &nbsp;&nbsp;&nbsp;&nbsp; Problem Solving By C Programming (ENGG1110N). <br />
</p>
</li>

</ul>


<h2>Resources</h2>
<p><i>Source Code</i></p>
<ul>

<li>
  <p><a href="https://github.com/Robert0812/deepsaldet">deepsaldet</a> : C++/Python code of our CVPR 2015 work "Saliency Detection by Multi-Context Deep Learning".</p>
</li>

<li>
  <p><a href="https://github.com/Robert0812/midfilter_reid">midfilter_reid</a> : MATLAB code of our CVPR 2014 work "Learning Mid-level Filters for Person Re-identification".</p>
</li>

<li>
  <p><a href="https://github.com/Robert0812/salience_match">salience_match</a> : MATLAB code of our ICCV 2013 work "Person Re-identification by Salience Matching".</p>
</li>

<li>
  <p><a href="https://github.com/Robert0812/salience_reid">salience_reid</a> : MATLAB code of our CVPR 2013 work "Unsupervised Salience Learning for Person Re-identification".</p>
</li>

<li>
  <p><a href="https://github.com/Robert0812/dense_feat">dense_feat</a> : MATLAB code of dense feature extractor used in our person re-identification works.</p>
</li>


</ul>

<!-- <h2>Datasets</h2> -->
<p><i>Datasets</i></p>
<ul>
<li>
  <p>
    <a href="https://docs.google.com/spreadsheet/viewform?formkey=dF9pZ1BFZkNiMG1oZUdtTjZPalR0MGc6MA">CUHK01 Dataset</a> : Person re-id dataset with 3, 884 images of 972 pedestrians. All pedestrian images are manually cropped, and normalized to 160 x 60 pixel.
    </p>
</li>
<li>
  <p>
    <a href="https://docs.google.com/spreadsheet/viewform?usp=drive_web&formkey=dHZtSGIwTnVDUEdWMFktQWU2bTZ0N3c6MA#gid=0
">CUHK02 Dataset</a> : Person re-id dataset with five camera view settings. This dataset is used for evaluating re-id algorithms under different camera view transforms. 
  </p>
</li>

<li>
  <p>
    <a href="https://docs.google.com/spreadsheet/viewform?usp=drive_web&formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0">CUHK03 Dataset</a> : Person re-id dataset with 13, 164 images of 1, 360 pedestrians. This dataset provides both manually cropped pedestrian images and auto detected ones (using prevailing pedestrian detector).
    </p>
</li>
</ul>

<!-- <h2>Links
</h2> -->
<p><i>Links</i></p>

<ul>
  <li>
    <a href="http://mmlab.ie.cuhk.edu.hk/project_deep_learning.html">A collection of deep learning materails</a>
  </li>

  <li>
    <a href="https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/resources">Deep Learning Course </a> offered by Prof. Xiaogang Wang 
  </li>

  <li>
    <a href="https://github.com/lindahua/MLPI">Machine Learning Course</a> offered by Prof. Dahua Lin
  </li>

  <li>
    <a href="http://www-labs.iro.umontreal.ca/~bengioy/DLbook/">Deep Learning Book</a> online draft written by Yoshua Bengio, Ian Goodfellow, and Aaron Courville.
  </li>

</ul>

<p>
  <a href="http://www.cuhk.edu.hk/english/index.html"><img src="figures/cuhk.jpg" alt="alt text" width="70px" height="HEIGHTpx"/></a>

  <a href="http://en.ustc.edu.cn/"><img src="figures/ustc.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://bashu.cn/"><img src="figures/bashu.jpg" alt="alt text" width="50px" height="50px"/></a>
</p>

<div id="footer">
<div id="footer-text">
<strong>Hi, you are the 
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=8960957; 
var sc_invisible=0; 
var sc_security="7339caeb"; 
var sc_text=2; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="hits counter"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="http://c.statcounter.com/8960957/0/7339caeb/0/"
alt="hits counter"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
-th visitor since 2013-05-15.</strong> </br>Last updated at 2015-01-08 by Rui Zhao. Page created using <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>


</body>
</html>
